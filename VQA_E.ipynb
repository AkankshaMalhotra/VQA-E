{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VQA-E.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5iGICGfmslk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1VdHLMxmuEm",
        "colab_type": "code",
        "outputId": "de6a84e7-664c-43cb-9f3e-cfcd6ef5d099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "\n",
        "################################################################################\n",
        "# Purpose: This code is for the implementation of the VAQ-E, that is given the #\n",
        "# question and answer, it generates answer from the image and also gives       #\n",
        "# textual explaination  In this part, I imported important liberaries, set TPU #\n",
        "# instructions, mounted the drive, downloaded image folder and read the file   #\n",
        "# names                                                                        #\n",
        "#                                                                              #\n",
        "# Disclaimer: No part of the code is copied from anywhere, I just have         #\n",
        "# followed Tensorflow tutorials wherever I was stuck.                          # \n",
        "#                                                                              #\n",
        "#                                                                              #\n",
        "# Author: Akanksha Malhotra                                                    #\n",
        "# Date: 20 April 2020                                                          #\n",
        "#                                                                              #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import cv2\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.92.83.178:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.92.83.178:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQkRy49Nm09c",
        "colab_type": "code",
        "outputId": "95e3cae5-852d-48c3-ed7c-0b0aec63802a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7pJQRcwm3pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open(\"/content/drive/My Drive/vqa/VQA-E-ComplementaryImages_train.json\", \"r\") as read_file:\n",
        "    training_data = json.load(read_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_puWd1hnF21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3cba8e84-d905-4be5-b663-c6a0b1de3d50"
      },
      "source": [
        "name_of_zip = 'train2014.zip'\n",
        "if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n",
        "  image_zip = tf.keras.utils.get_file(name_of_zip,\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip)+'/train2014/'\n",
        "else:\n",
        "\n",
        "  PATH = os.path.abspath('.')+'/train2014/'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 264s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOoIlj1UpRP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = os.listdir(\"/content/train2014\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKJh9x0BnNhi",
        "colab_type": "code",
        "outputId": "d0f5745e-8512-4543-f5e2-a2f960acac59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "################################################################################\n",
        "# Purpose: Reads the training data and gets full path for the image, answers,  # \n",
        "# questions, complementary image path, and explanation. There were some images #\n",
        "# names in VQA-E dataset that were not available in train14 folder, image data #\n",
        "# folder downloaded from MSCOCO.                                               #\n",
        "#                                                                              #\n",
        "# Author: Akanksha Malhotra                                                    #\n",
        "# Date: 21 April 2020                                                          #\n",
        "#                                                                              #\n",
        "################################################################################\n",
        "\n",
        "from tqdm import tqdm\n",
        "questions  = []\n",
        "answers = []\n",
        "images = []\n",
        "mcq = []\n",
        "question_type = []\n",
        "answer_type = []\n",
        "explanation = []\n",
        "complementary_pairs = []\n",
        "from collections import defaultdict\n",
        "image_path = []\n",
        "all_answers = []\n",
        "for i in tqdm(training_data):\n",
        "    full_coco_image_path = \"/content/train2014/\" + 'COCO_train2014_' + '%012d.jpg' % (i[\"img_id\"])\n",
        "    c = 'COCO_train2014_' + '%012d.jpg' % (i[\"img_id\"])\n",
        "    if c not in x:\n",
        "      continue\n",
        "    d = defaultdict(int)\n",
        "    for j in i[\"answers\"]:\n",
        "      d[j] += 1\n",
        "    f=0\n",
        "    for a in d.items():\n",
        "      if a[1]>=8:\n",
        "        v = a[0]\n",
        "        f=1\n",
        "    if f==0:\n",
        "      continue\n",
        "    else:\n",
        "      all_answers.append(v)  \n",
        "    questions.append(i['question'])\n",
        "    images.append(i[\"img_id\"])\n",
        "    complementary_pairs.append(i[\"comp_img_id\"])\n",
        "    answers.append(max(d.items(), key=lambda x: x[1])[0])\n",
        "    answer_type.append(i[\"answer_type\"] )\n",
        "    question_type.append(i[\"question_type\"])\n",
        "    mcq.append(i[\"multiple_choice_answer\"])\n",
        "    explanation.append(i[\"explanation\"][0])\n",
        "    image_path.append(full_coco_image_path)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 160656/160656 [01:50<00:00, 1460.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bYeplJNrTpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_answers = set(all_answers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuWCdCPyn5KP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "################################################################################\n",
        "# Purpose: Shuffling the dataset to bring randomness in the data               # \n",
        "# Author: Akanksha Malhotra                                                    #\n",
        "#                                                                              #\n",
        "################################################################################\n",
        "train_answers, train_questions, img_name_vector,explaination, all_complementary_pairs = shuffle(answers,questions,\n",
        "                                          image_path,explanation, complementary_pairs,\n",
        "                                          random_state=1)\n",
        "\n",
        "# selecting the first 20000 examples from the shuffled set\n",
        "num_examples = 1000\n",
        "train_answers = train_answers[:num_examples]\n",
        "train_questions = train_questions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]\n",
        "all_complementary_pairs = all_complementary_pairs[:num_examples]\n",
        "explaination = explaination[:num_examples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl5HHMMen_jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "def load_glove_model(glove_file):\n",
        "    \"\"\"\n",
        "    Given the path to the glove vector, it loads the glove embeddings\n",
        "    Author: Akanksha Malhotra\n",
        "    :param glove_file: embeddings_path: path of glove file.\n",
        "    :return: glove model\n",
        "    \"\"\"\n",
        "\n",
        "    f = codecs.open(glove_file + \".txt\", 'r', encoding='utf-8')\n",
        "    model = {}\n",
        "    for line in f:\n",
        "        split_line = line.split()\n",
        "        word = split_line[0]\n",
        "        embedding = np.array([float(val) for val in split_line[1:]])\n",
        "        model[word] = embedding\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYYPQMjhoAmM",
        "colab_type": "code",
        "outputId": "ac954b0c-312a-4292-ee57-5d48c98b7fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "def calc_max_length(tensor):\n",
        "  ''' \n",
        "  Given the tensor, it returns maximum length of the sequence within the tensor\n",
        "  Author: Akanksha Malhotra\n",
        "  :param: tensor\n",
        "  :return: max length of the sequence\n",
        "  '''\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "################################################################################\n",
        "# Purpose: For the questions in the dataset, it does preprocessing of the text.#\n",
        "# The vocabulary is top 5000 words. Padding is added to generated sequence.    #\n",
        "# The sequence of words is converted to sequence of numbers using word2index   #\n",
        "# dictionary. 1 is added to vocab length to keep track of padding that we have #\n",
        "# added.                                                                       #\n",
        "#                                                                              #\n",
        "# Author: Akanksha Malhotra                                                    #\n",
        "################################################################################\n",
        "top_k1 = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k1,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_questions)\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "ques_vocab = top_k1+1\n",
        "\n",
        "train_question_seqs = tokenizer.texts_to_sequences(train_questions)\n",
        "\n",
        "question_vector = tf.keras.preprocessing.sequence.pad_sequences(train_question_seqs, padding='post')\n",
        "\n",
        "max_length = calc_max_length(train_question_seqs)\n",
        "print(max_length)\n",
        "\n",
        "#new edit\n",
        "max_q = max_length"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<unk>': 1, 'the': 2, 'is': 3, 'what': 4, 'are': 5, 'this': 6, 'in': 7, 'on': 8, 'a': 9, 'many': 10, 'how': 11, 'color': 12, 'of': 13, 'there': 14, 'man': 15, 'picture': 16, 'people': 17, 'animal': 18, 'wearing': 19, 'these': 20, 'does': 21, 'room': 22, 'playing': 23, 'animals': 24, 'person': 25, 'doing': 26, 'bus': 27, 'kind': 28, 'plate': 29, 'sport': 30, 'or': 31, 'holding': 32, 'it': 33, 'photo': 34, 'cat': 35, 'dog': 36, 'to': 37, 'they': 38, 'have': 39, 'woman': 40, 'type': 41, 'boy': 42, 'that': 43, 'sitting': 44, 'shown': 45, 'plane': 46, 'pizza': 47, 'ground': 48, 'water': 49, 'girl': 50, 'train': 51, 'at': 52, 'sign': 53, 'an': 54, 'white': 55, 'being': 56, 'horses': 57, 'can': 58, 'he': 59, 'game': 60, 'computer': 61, 'any': 62, 'seen': 63, 'elephants': 64, 'standing': 65, 'with': 66, 'shirt': 67, 'table': 68, 'be': 69, 'sky': 70, 'toilet': 71, 'hand': 72, 'bird': 73, 'zebras': 74, 'do': 75, \"man's\": 76, 'cake': 77, 'you': 78, 'her': 79, 'hat': 80, 'she': 81, 'riding': 82, 'left': 83, 'his': 84, 'wall': 85, 'clean': 86, 'guy': 87, 'clock': 88, 'black': 89, 'red': 90, 'number': 91, 'child': 92, 'looking': 93, 'say': 94, 'bike': 95, 'bed': 96, 'name': 97, 'up': 98, 'taking': 99, 'boats': 100, 'same': 101, 'building': 102, 'pictured': 103, 'top': 104, 'air': 105, 'giraffes': 106, 'and': 107, 'bear': 108, 'played': 109, 'vehicle': 110, 'did': 111, 'see': 112, 'men': 113, 'where': 114, 'ball': 115, 'than': 116, 'moving': 117, 'sandwich': 118, 'day': 119, 'glasses': 120, 'skateboard': 121, 'lights': 122, 'bathroom': 123, 'which': 124, 'street': 125, 'object': 126, 'eating': 127, 'all': 128, 'side': 129, 'vegetable': 130, 'cow': 131, 'skiing': 132, 'birds': 133, 'fall': 134, 'phone': 135, 'above': 136, 'walking': 137, 'head': 138, 'front': 139, 'food': 140, 'going': 141, 'trains': 142, 'keyboard': 143, 'made': 144, 'car': 145, 'bench': 146, 'out': 147, 'sink': 148, 'for': 149, 'grass': 150, 'kids': 151, 'one': 152, 'zebra': 153, 'smiling': 154, 'window': 155, 'truck': 156, 'pic': 157, 'luggage': 158, \"person's\": 159, 'back': 160, 'happy': 161, 'fire': 162, 'hydrant': 163, 'boat': 164, 'laying': 165, 'snow': 166, 'door': 167, 'daytime': 168, 'right': 169, 'someone': 170, 'off': 171, 'zoo': 172, 'over': 173, 'green': 174, 'camera': 175, 'children': 176, 'sleeping': 177, 'seat': 178, 'banana': 179, 'vase': 180, 'vegetarian': 181, 'touching': 182, 'floor': 183, 'surfboard': 184, 'umbrella': 185, 'hot': 186, 'tower': 187, 'laptops': 188, 'bears': 189, 'stripe': 190, 'face': 191, 'flying': 192, 'real': 193, 'screens': 194, 'house': 195, 'was': 196, 'meat': 197, 'english': 198, 'dirty': 199, \"what's\": 200, 'carrying': 201, 'elephant': 202, 'present': 203, 'glass': 204, 'party': 205, 'old': 206, 'shirts': 207, 'stuffed': 208, 'has': 209, 'orange': 210, 'lit': 211, 'called': 212, 'light': 213, 'dogs': 214, 'kitchen': 215, 'baby': 216, 'kid': 217, 'who': 218, 'bottom': 219, 'item': 220, 'direction': 221, 'utensil': 222, 'yellow': 223, 'planes': 224, 'apples': 225, 'other': 226, 'girls': 227, 'tennis': 228, 'both': 229, 'road': 230, 'wild': 231, 'sheets': 232, 'horse': 233, 'motorcycle': 234, 'image': 235, 'visible': 236, 'here': 237, 'feet': 238, 'laptop': 239, 'shop': 240, \"horse's\": 241, 'mane': 242, 'cows': 243, 'wetsuit': 244, 'eat': 245, 'park': 246, 'little': 247, 'transportation': 248, 'fridge': 249, 'using': 250, 'closed': 251, 'thrown': 252, 'apple': 253, 'match': 254, 'shelf': 255, 'fruit': 256, 'main': 257, 'stands': 258, 'women': 259, 'like': 260, 'painted': 261, 'outside': 262, 'two': 263, 'ready': 264, 'trees': 265, 'taken': 266, 'phones': 267, 'desk': 268, 'character': 269, 'pants': 270, 'mouse': 271, 'next': 272, 'ski': 273, 'lift': 274, 'could': 275, 'candles': 276, 'stop': 277, 'shorts': 278, 'lady': 279, 'brand': 280, 'event': 281, 'cold': 282, 'toothbrush': 283, 'cats': 284, 'middle': 285, 'church': 286, 'walls': 287, 'flight': 288, 'scene': 289, 'tie': 290, 'deep': 291, 'hanging': 292, 'adult': 293, 'near': 294, 'inside': 295, 'refrigerator': 296, 'hands': 297, 'skateboards': 298, 'home': 299, 'office': 300, 'oven': 301, 'open': 302, 'passenger': 303, 'landing': 304, 'behind': 305, 'double': 306, 'decker': 307, 'more': 308, 'pepperoni': 309, 'blue': 310, 'down': 311, 'working': 312, 'throw': 313, 'propellers': 314, 'slices': 315, 'we': 316, 'signs': 317, 'tree': 318, 'wine': 319, 'taller': 320, 'donuts': 321, 'talking': 322, 'alone': 323, 'benches': 324, 'cup': 325, 'kickstand': 326, 'pizzas': 327, \"elephant's\": 328, 'sheep': 329, 'facing': 330, 'professional': 331, 'shower': 332, 'vegetables': 333, 'dirt': 334, 'raining': 335, 'motion': 336, 'well': 337, 'mirror': 338, 'something': 339, 'chairs': 340, 'ducks': 341, 'restaurant': 342, 'flowers': 343, 'field': 344, 'tv': 345, 'larger': 346, 'most': 347, 'from': 348, 'depicted': 349, 'motorcycles': 350, 'clouds': 351, 'drinking': 352, 'participating': 353, 'giraffe': 354, 'headlights': 355, 'sunny': 356, 'running': 357, 'suitcase': 358, 'boys': 359, 'legs': 360, 'windows': 361, \"cat's\": 362, 'fur': 363, 'each': 364, 'leash': 365, 'just': 366, 'parked': 367, 'puppy': 368, 'meters': 369, 'throwing': 370, 'about': 371, 'breed': 372, 'young': 373, 'egg': 374, 'clear': 375, 'market': 376, 'selfie': 377, 'snowboarder': 378, 'midair': 379, 'roses': 380, 'airplane': 381, 'propeller': 382, 'busy': 383, 'lot': 384, 'redhead': 385, 'hitchhiking': 386, \"girl's\": 387, 'forks': 388, 'upright': 389, 'size': 390, 'pink': 391, 'shining': 392, 'pasta': 393, 'frosting': 394, 'part': 395, 'plants': 396, 'pillow': 397, 'passing': 398, 'shoes': 399, 'bananas': 400, 'leaves': 401, 'school': 402, 'accessory': 403, 'balding': 404, 'outdoors': 405, 'earring': 406, 'new': 407, 'chair': 408, 'toppings': 409, 'dry': 410, 'donut': 411, 'displayed': 412, 'television': 413, 'shiny': 414, 'silver': 415, 'dishes': 416, 'catching': 417, 'liquid': 418, 'waiting': 419, 'laid': 420, 'look': 421, 'bathtub': 422, 'lines': 423, 'brushes': 424, 'shoelaces': 425, 'trash': 426, 'tucked': 427, 'slope': 428, 'chicken': 429, 'broccoli': 430, 'cluttered': 431, 'wastebasket': 432, 'helmet': 433, \"skater's\": 434, 'writing': 435, 'cord': 436, 'blazer': 437, 'pitcher': 438, \"sheep's\": 439, 'waiter': 440, 'enjoy': 441, 'beach': 442, 'couch': 443, 'doors': 444, 'ripe': 445, 'trail': 446, 'belong': 447, 'ribbon': 448, 'around': 449, 'wires': 450, 'pushing': 451, 'family': 452, 'counter': 453, 'filled': 454, 'place': 455, 'gentlemen': 456, 'sun': 457, 'set': 458, 'tables': 459, 'position': 460, 'play': 461, 'tagged': 462, 'traffic': 463, 'cross': 464, 'country': 465, 'sort': 466, 'piece': 467, 'clothing': 468, 'bow': 469, 'curtains': 470, 'work': 471, 'p': 472, 'machine': 473, 'mean': 474, 'freight': 475, 'ketchup': 476, 'dozing': 477, 'hungry': 478, 'male': 479, 'barbie': 480, 'sports': 481, 'foods': 482, 'healthy': 483, 'subway': 484, 'guys': 485, 'asleep': 486, 'hitting': 487, 'swimming': 488, 'salad': 489, 'engaged': 490, 'hygiene': 491, 'coat': 492, 'mobile': 493, 'flip': 494, 'flooring': 495, 'company': 496, 'sidewalks': 497, 'night': 498, 'israel': 499, 'advertisement': 500, 'journey': 501, 'wireless': 502, 'bedding': 503, 'reflective': 504, 'river': 505, 'lake': 506, 'aircraft': 507, 'statue': 508, 'competing': 509, 'topping': 510, 'vest': 511, 'bag': 512, 'skater': 513, 'buses': 514, 'commercial': 515, 'relaxed': 516, 'model': 517, 'tub': 518, 'foil': 519, 'protection': 520, 'boxing': 521, 'under': 522, 'scarf': 523, 'canoe': 524, 'cap': 525, 'flower': 526, 'pattern': 527, 'french': 528, 'jeans': 529, 'flags': 530, 'toy': 531, 'upside': 532, 'toys': 533, 'sliced': 534, 'wet': 535, 'multiple': 536, 'tracks': 537, 'feeling': 538, 'bowl': 539, 'batter': 540, 'brownie': 541, 'anyone': 542, 'lego': 543, 'scooter': 544, 'kissing': 545, \"pitcher's\": 546, 'last': 547, 'blanket': 548, 'condiments': 549, 'skier': 550, 'liquor': 551, 'catcher': 552, 'get': 553, 'language': 554, 'colored': 555, 'teal': 556, 'eyes': 557, 'growing': 558, 'bread': 559, 'toilets': 560, 'viewing': 561, \"people's\": 562, 'heads': 563, 'bikes': 564, 'handles': 565, 'scissors': 566, 'harbor': 567, 'hotdog': 568, 'fit': 569, 'bun': 570, 'huge': 571, 'few': 572, 'cars': 573, 'usual': 574, 'bottle': 575, 'camo': 576, 'jumping': 577, 'specialty': 578, 'skating': 579, 'wheels': 580, 'teapot': 581, 'would': 582, 'placemat': 583, 'no': 584, 'players': 585, 'mode': 586, 'wooden': 587, 'ears': 588, 'sticking': 589, 'mouth': 590, 'underneath': 591, \"bear's\": 592, 'dressed': 593, 'dish': 594, 'winter': 595, 'horns': 596, 'featured': 597, 'bento': 598, 'box': 599, 'turned': 600, 'surfer': 601, 'snack': 602, 'been': 603, 'eaten': 604, 'rose': 605, 'baseball': 606, 'player': 607, 'pitching': 608, 'tusks': 609, 'reach': 610, 'ice': 611, 'maker': 612, 'dessert': 613, 'buffet': 614, 'curtain': 615, 'stove': 616, 'electric': 617, 'body': 618, 'fishing': 619, 'bedspread': 620, 'headboard': 621, 'avenue': 622, 'chasing': 623, 'competition': 624, 'pillows': 625, 'lush': 626, 'done': 627, 'hangs': 628, 'numbers': 629, 'him': 630, 'habitat': 631, 'kitty': 632, 'beard': 633, '81': 634, 'flavor': 635, 'desert': 636, 'backsplash': 637, 'sunglasses': 638, 'apartment': 639, 'kept': 640, 'area': 641, 'full': 642, 'jacket': 643, 'comforter': 644, 'sidewalk': 645, 'video': 646, 'also': 647, \"someone's\": 648, 'birthday': 649, 'sit': 650, '18': 651, 'giving': 652, 'shape': 653, 'good': 654, 'skateboarding': 655, 'flushed': 656, 'uniform': 657, 'some': 658, 'thirsty': 659, 'sticker': 660, 'seem': 661, 'halved': 662, 'tasty': 663, 'everyone': 664, 'channel': 665, 'kitchens': 666, 'seats': 667, 'vases': 668, 'them': 669, 'hoses': 670, 'meal': 671, 'airline': 672, 'flies': 673, 'planning': 674, 'bite': 675, 'safely': 676, 'ride': 677, 'domesticated': 678, 'six': 679, 'pulling': 680, 'foreground': 681, 'background': 682, \"bus's\": 683, 'post': 684, 'frosted': 685, 'without': 686, 'longer': 687, 'court': 688, 'clocks': 689, 'primarily': 690, 'three': 691, 'skis': 692, 'missing': 693, 'peeled': 694, 'trick': 695, 'posing': 696, 'celebrated': 697, 'monitors': 698, 'slim': 699, 'high': 700, 'reflection': 701, 'cloudy': 702, 'carnival': 703, 'prizes': 704, 'lap': 705, 'time': 706, 'balanced': 707, 'documents': 708, 'crash': 709, 'petting': 710, \"it's\": 711, 'waves': 712, 'highway': 713, 'overcast': 714, \"dog's\": 715, 'mountains': 716, 'completely': 717, 'tiled': 718, 'trucks': 719, 'plates': 720, 'milk': 721, 'faces': 722, 'lazy': 723, 'fence': 724, 'lead': 725, 'things': 726, 'active': 727, 'messy': 728, 'sand': 729, 'smooth': 730, 'sailboat': 731, 'desktop': 732, 'dangerous': 733, 'brick': 734, 'microwave': 735, 'wrote': 736, 'strawberries': 737, 'length': 738, 'forefront': 739, 'knife': 740, 'balls': 741, 'different': 742, 'types': 743, 'teddy': 744, 'brown': 745, 'reading': 746, 'books': 747, 'crane': 748, 'conditioned': 749, 'bathing': 750, 'suit': 751, 'tomatoes': 752, 'damp': 753, 'wrapped': 754, 'broken': 755, 'attached': 756, 'device': 757, 'functioning': 758, 'staining': 759, 'foot': 760, 'leg': 761, 'dinnerware': 762, 'plant': 763, 'smaller': 764, 'collar': 765, 'picnic': 766, 'system': 767, 'resting': 768, 'parking': 769, 'colorful': 770, 'big': 771, 'sweater': 772, 'cliff': 773, 'snowboarding': 774, 'rooftop': 775, 'engines': 776, 'smallest': 777, 'covers': 778, \"plane's\": 779, 'gear': 780, 'burning': 781, 'snowboards': 782, 'catch': 783, 'curving': 784, 'handstand': 785, 'holiday': 786, 'feather': 787, 'pickup': 788, 'will': 789, 'surrounding': 790, 'surfing': 791, 'screen': 792, 'utensils': 793, 'cooking': 794, 'asian': 795, 'ceiling': 796, 'island': 797, 'doughnut': 798, 'sell': 799, 'hats': 800, 'forward': 801, 'pole': 802, 'captivity': 803, 'toppers': 804, 'tight': 805, 'hotel': 806, 'poser': 807, \"lady's\": 808, 'couple': 809, 'drink': 810, 'landforms': 811, 'commodes': 812}\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHM9rrb8nxmp",
        "colab_type": "code",
        "outputId": "923133b1-7d2a-42d6-c794-b064ee892119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# considering all answers to be part of ans vocab\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Purpose: creates the label for each answer and creates answer vocab\n",
        "#  \n",
        "# Author: Akanksha Malhotra\n",
        "###############################################################################\n",
        "\n",
        "# define example\n",
        "data = train_answers\n",
        "values = array(data)\n",
        "print(values[:10])\n",
        "\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "\n",
        "#new edit\n",
        "ans_vocab = {l: i for i, l in enumerate(label_encoder.classes_)}\n",
        "\n",
        "answer_vector = integer_encoded\n",
        "print(answer_vector[:10])\n",
        "len_ans_vocab = len(ans_vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cat' '2' 'yes' 'yes' 'sandwich' 'white' 'brown' 'frisbee' 'yes' 'no']\n",
            "[ 43   2 191 191 138 185  34  73 191 115]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvasEuBtoOau",
        "colab_type": "code",
        "outputId": "88ffdd69-3d46-48af-fff2-a776fbc9f89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "################################################################################\n",
        "# Purpose: For the explanations in the dataset, it does preprocessing of the   #\n",
        "# text. The vocabulary is top 5000 words. Padding is added to generated        #\n",
        "# sequence.                                                                    #\n",
        "# The sequence of words is converted to sequence of numbers using word2index   #\n",
        "# dictionary. 1 is added to vocab length to keep track of padding that we have #\n",
        "# added.                                                                       #\n",
        "#                                                                              #\n",
        "# Author: Akanksha Malhotra                                                    #\n",
        "################################################################################\n",
        "\n",
        "top_k = 5000\n",
        "tokenizer1 = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer1.fit_on_texts(explaination)\n",
        "train_explaination_seqs = tokenizer1.texts_to_sequences(explaination)\n",
        "\n",
        "tokenizer1.word_index['<pad>'] = 0\n",
        "tokenizer1.index_word[0] = '<pad>'\n",
        "\n",
        "\n",
        "train_explaination_seqs = tokenizer1.texts_to_sequences(explaination)\n",
        "\n",
        "\n",
        "explaination_vector = tf.keras.preprocessing.sequence.pad_sequences(train_explaination_seqs, padding='post')\n",
        "print(tokenizer1.word_index)\n",
        "explaination_vocab = top_k+1\n",
        "max_length1 = calc_max_length(train_explaination_seqs)\n",
        "print(max_length1)\n",
        "\n",
        "#new edit\n",
        "max_q1 = max_length1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<unk>': 1, 'a': 2, 'the': 3, 'is': 4, 'on': 5, 'in': 6, 'of': 7, 'with': 8, 'and': 9, 'are': 10, 'two': 11, 'man': 12, 'that': 13, 'to': 14, 'white': 15, 'sitting': 16, 'an': 17, 'standing': 18, 'at': 19, 'people': 20, 'there': 21, 'it': 22, 'next': 23, 'black': 24, 'woman': 25, 'holding': 26, 'plate': 27, 'table': 28, 'his': 29, 'street': 30, 'red': 31, 'blue': 32, 'down': 33, 'field': 34, 'this': 35, 'kitchen': 36, 'bathroom': 37, 'front': 38, 'bus': 39, 'playing': 40, 'some': 41, 'young': 42, 'has': 43, 'one': 44, 'large': 45, 'person': 46, 'cat': 47, 'dog': 48, 'top': 49, 'pizza': 50, 'grass': 51, 'green': 52, 'three': 53, 'baseball': 54, 'boy': 55, 'bed': 56, 'by': 57, 'men': 58, 'riding': 59, 'near': 60, 'wearing': 61, 'train': 62, 'frisbee': 63, 'up': 64, 'for': 65, 'while': 66, 'water': 67, 'tennis': 68, 'skateboard': 69, 'parked': 70, 'sign': 71, 'small': 72, 'snow': 73, 'air': 74, 'room': 75, 'other': 76, 'laptop': 77, 'looking': 78, 'walking': 79, 'group': 80, 'flying': 81, 'laying': 82, 'beach': 83, 'zebras': 84, 'little': 85, 'side': 86, 'clock': 87, 'cake': 88, 'from': 89, 'four': 90, 'shirt': 91, 'ball': 92, 'giraffe': 93, 'game': 94, 'yellow': 95, 'trees': 96, 'elephant': 97, 'bear': 98, 'sink': 99, 'toilet': 100, 'picture': 101, 'computer': 102, 'horse': 103, 'girl': 104, 'building': 105, 'open': 106, 'bench': 107, 'horses': 108, 'several': 109, 'women': 110, 'desk': 111, 'together': 112, 'brown': 113, 'park': 114, 'be': 115, 'as': 116, 'elephants': 117, 'tree': 118, 'food': 119, 'many': 120, 'eating': 121, 'airplane': 122, 'floor': 123, 'wall': 124, 'camera': 125, 'motorcycle': 126, 'into': 127, 'out': 128, 'window': 129, 'outside': 130, 'road': 131, 'sandwich': 132, 'pink': 133, 'board': 134, 'covered': 135, 'around': 136, 'through': 137, 'sits': 138, 'couple': 139, 'city': 140, 'ocean': 141, 'ready': 142, 'corner': 143, 'cow': 144, 'umbrella': 145, 'her': 146, 'player': 147, 'day': 148, 'parking': 149, 'baby': 150, 'mirror': 151, 'stop': 152, 'driving': 153, 'middle': 154, 'area': 155, 'counter': 156, 'their': 157, 'hand': 158, 'lot': 159, 'tracks': 160, 'its': 161, 'wine': 162, 'shower': 163, 'orange': 164, 'back': 165, 'over': 166, 'old': 167, 'photo': 168, 'who': 169, 'under': 170, 'slope': 171, 'ski': 172, 'court': 173, 'dirt': 174, 'lying': 175, 'working': 176, \"it's\": 177, 'background': 178, 'light': 179, 'glass': 180, 'each': 181, 'kites': 182, 'head': 183, 'suitcase': 184, 'fire': 185, 'bowl': 186, 'tie': 187, 'hat': 188, 'stands': 189, 'hot': 190, 'plane': 191, 'keyboard': 192, 'watching': 193, 'kite': 194, 'child': 195, 'enclosure': 196, 'glasses': 197, 'posing': 198, 'living': 199, 'kids': 200, 'looks': 201, 'phone': 202, 'broccoli': 203, 'behind': 204, 'boat': 205, 'pulling': 206, 'door': 207, 'beside': 208, 'giraffes': 209, 'ground': 210, 'during': 211, 'slice': 212, 'sleeping': 213, 'hydrant': 214, 'long': 215, 'lady': 216, 'getting': 217, 'television': 218, 'cows': 219, 'suit': 220, 'box': 221, 'airport': 222, 'stand': 223, 'he': 224, 'hill': 225, 'about': 226, 'off': 227, 'tower': 228, 'stove': 229, 'birds': 230, 'hit': 231, 'skiing': 232, 'dark': 233, 'shown': 234, 'can': 235, 'skis': 236, 'screen': 237, 'sheep': 238, 'truck': 239, 'sit': 240, 'grassy': 241, 'sky': 242, 'taking': 243, 'full': 244, 'surfboard': 245, 'snowy': 246, 'tall': 247, 'banana': 248, 'cars': 249, 'image': 250, 'cut': 251, 'big': 252, 'another': 253, 'bunch': 254, 'hanging': 255, 'not': 256, 'cheese': 257, 'going': 258, 'which': 259, 'appears': 260, 'moving': 261, 'cats': 262, 'couch': 263, 'house': 264, 'wooden': 265, 'talking': 266, 'pair': 267, 'night': 268, 'along': 269, 'fence': 270, 'vegetables': 271, 'bird': 272, 'cell': 273, 'snowboard': 274, 'refrigerator': 275, 'double': 276, 'decker': 277, 'children': 278, 'stuffed': 279, 'brick': 280, 'bike': 281, 'doing': 282, 'sand': 283, 'sidewalk': 284, 'dressed': 285, 'clouds': 286, 'vase': 287, 'him': 288, 'buildings': 289, 'different': 290, 'inside': 291, 'surf': 292, 'lights': 293, 'curb': 294, 'apple': 295, 'bananas': 296, 'wood': 297, 'monitor': 298, 'chair': 299, 'catch': 300, 'cup': 301, 'computers': 302, 'pitcher': 303, 'signs': 304, 'coffee': 305, 'family': 306, 'trying': 307, 'rock': 308, 'adult': 309, 'guy': 310, 'set': 311, 'yard': 312, 'rain': 313, 'empty': 314, 'video': 315, 'way': 316, 'meal': 317, 'running': 318, 'like': 319, 'bottle': 320, 'flowers': 321, 'trick': 322, 'boys': 323, 'all': 324, 'racquet': 325, 'older': 326, 'mountain': 327, 'station': 328, 'boats': 329, 'docked': 330, 'half': 331, 'bottom': 332, 'piece': 333, 'hands': 334, 'throws': 335, 'no': 336, 'curtain': 337, 'lake': 338, 'airliner': 339, 'shelf': 340, 'home': 341, 'hay': 342, 'luggage': 343, 'swings': 344, 'number': 345, 'view': 346, 'shows': 347, 'row': 348, 'cross': 349, 'pizzas': 350, 'soccer': 351, 'helmet': 352, 'mouse': 353, 'smiling': 354, 'lined': 355, 'cutting': 356, 'made': 357, 'plates': 358, 'teddy': 359, 'distance': 360, 'very': 361, 'eaten': 362, 'brushing': 363, 'teeth': 364, 'brush': 365, 'catcher': 366, 'market': 367, 'walk': 368, 'partially': 369, 'traveling': 370, 'shorts': 371, 'bag': 372, 'salad': 373, 'busy': 374, 'oven': 375, 'above': 376, 'dogs': 377, 'passenger': 378, 'jet': 379, 'coming': 380, 'grazing': 381, 'tray': 382, 'six': 383, 'between': 384, 'bicycle': 385, 'bedroom': 386, 'apples': 387, 'bright': 388, 'clean': 389, 'animal': 390, 'stone': 391, 'animals': 392, 'school': 393, 'surfing': 394, 'carrots': 395, 'clothes': 396, 'bun': 397, 'have': 398, 'grey': 399, 'wii': 400, 'colored': 401, 'towards': 402, 'toppings': 403, 'time': 404, 'hair': 405, 'surfer': 406, 'wave': 407, 'pretty': 408, 'skateboarder': 409, 'fork': 410, 'bat': 411, 'laptops': 412, 'or': 413, 'rocks': 414, 'closed': 415, 'reading': 416, 'against': 417, 'book': 418, 'across': 419, 'they': 420, 'pillow': 421, 'onions': 422, 'using': 423, 'donut': 424, 'pole': 425, 'crowd': 426, 'waiting': 427, 'team': 428, 'watch': 429, 'professional': 430, 'batter': 431, 'vehicles': 432, 'including': 433, 'foreground': 434, 'towels': 435, 'country': 436, 'purple': 437, 'candles': 438, 'birthday': 439, 'seat': 440, 'jeans': 441, 'paper': 442, 'outdoors': 443, 'christmas': 444, 'bikes': 445, 'rack': 446, 'tub': 447, 'chairs': 448, 'dock': 449, 'ribbon': 450, 'pick': 451, 'end': 452, 'lit': 453, 'scissors': 454, 'cloudy': 455, 'wedding': 456, 'sticking': 457, 'jacket': 458, 'branch': 459, 'toy': 460, 'bread': 461, 'gray': 462, 'throwing': 463, 'drink': 464, 'drinking': 465, 'swinging': 466, 'cream': 467, 'walks': 468, 'tv': 469, 'being': 470, 'ties': 471, 'after': 472, 'peppers': 473, 'line': 474, 'zebra': 475, 'fridge': 476, 'racket': 477, 'them': 478, 'ice': 479, 'meat': 480, 'umbrellas': 481, 'something': 482, 'pieces': 483, 'zoo': 484, 'racing': 485, 'hotdog': 486, 'donuts': 487, 'benches': 488, 'showing': 489, 'goat': 490, 'seen': 491, 'buses': 492, 'display': 493, 'stream': 494, 'close': 495, 'jumping': 496, 'shaking': 497, 'games': 498, 'only': 499, 'sunny': 500, 'wild': 501, 'good': 502, 'lane': 503, 'tarmac': 504, 'huge': 505, 'fresh': 506, 'cabinets': 507, 'petting': 508, 'runway': 509, 'tied': 510, 'lawn': 511, 'hitting': 512, 'few': 513, 'nintendo': 514, 'controller': 515, 'pointing': 516, 'beneath': 517, 'flower': 518, 'preparing': 519, 'leave': 520, 'church': 521, 'look': 522, 'blonde': 523, 'crouching': 524, 'cement': 525, 'take': 526, 'locomotive': 527, 'crossing': 528, 'plastic': 529, 'broken': 530, 'kid': 531, 'leg': 532, 'setting': 533, 'sinks': 534, 'leaning': 535, 'newspaper': 536, 'leaves': 537, 'mustache': 538, 'purse': 539, 'carrying': 540, 'scene': 541, 'flock': 542, 'seems': 543, 'run': 544, 'what': 545, 'pie': 546, 'olives': 547, 'just': 548, 'destination': 549, 'similarly': 550, 'immediate': 551, 'skiers': 552, 'shape': 553, 'ducks': 554, 'sill': 555, 'brushes': 556, 'putting': 557, 'pasture': 558, 'shoes': 559, 'woods': 560, 'wireless': 561, 'laughing': 562, 'chicken': 563, 'tiled': 564, 'skateboarding': 565, 'serve': 566, 'wires': 567, 'skier': 568, 'couches': 569, 'overhead': 570, 'tank': 571, 'silver': 572, 'guys': 573, 'cellphone': 574, 'beer': 575, 'text': 576, 'van': 577, 'stoplight': 578, 'sun': 579, 'cart': 580, 'town': 581, 'tooth': 582, 'steps': 583, 'barbershop': 584, 'track': 585, 'subway': 586, 'things': 587, 'narrow': 588, 'underneath': 589, 'trains': 590, 'perched': 591, 'swing': 592, 'bearded': 593, 'frosting': 594, 'bowls': 595, 'coat': 596, 'sunglasses': 597, 'office': 598, 'performing': 599, 'location': 600, 'advertisement': 601, 'use': 602, 'turned': 603, 'skate': 604, 'bathtub': 605, 'surrounded': 606, 'trucks': 607, 'opened': 608, 'slices': 609, 'package': 610, 'falling': 611, 'pepperoni': 612, 'garden': 613, 'milk': 614, 'jersey': 615, 'she': 616, 'prepares': 617, 'car': 618, 'french': 619, 'river': 620, 'upside': 621, 'gear': 622, 'served': 623, 'past': 624, 'italian': 625, 'soup': 626, 'hills': 627, 'watches': 628, 'pitch': 629, 'stack': 630, 'propellers': 631, 'fries': 632, 'works': 633, 'horns': 634, 'toilets': 635, 'frisbees': 636, 'single': 637, 'enjoying': 638, 'shade': 639, 'plains': 640, 'bushes': 641, 'dish': 642, 'waves': 643, 'police': 644, 'shore': 645, 'seven': 646, 'chasing': 647, 'stainless': 648, 'steel': 649, 'built': 650, 'mountainside': 651, 'dirty': 652, 'fishing': 653, 'neatly': 654, 'sheets': 655, 'pillows': 656, 'valley': 657, 'metro': 658, 'swimming': 659, 'helping': 660, 'gas': 661, 'enter': 662, 'stairs': 663, 'dishwasher': 664, 'rear': 665, 'indoor': 666, 'picnic': 667, 'nice': 668, 'crust': 669, 'we': 670, 'see': 671, 'iron': 672, 'rackets': 673, 'mouth': 674, 'comforter': 675, 'itself': 676, 'friends': 677, 'asian': 678, 'snowboards': 679, 'swans': 680, 'before': 681, 'someone': 682, 'dishes': 683, 'various': 684, 'licking': 685, 'male': 686, 'fenced': 687, 'female': 688, 'store': 689, 'tricks': 690, 'pictures': 691, 'space': 692, 'outdoor': 693, 'fly': 694, 'high': 695, 'rice': 696, 'surfers': 697, 'saying': 698, 'includes': 699, 'ship': 700, 'arm': 701, 'items': 702, 'microwave': 703, 'pen': 704, 'leather': 705, 'sofa': 706, 'basket': 707, 'sweater': 708, 'crane': 709, 'clear': 710, 'right': 711, 'smaller': 712, 'leash': 713, 'bears': 714, 'fish': 715, 'control': 716, 'play': 717, 'meter': 718, 'outfit': 719, 'mountains': 720, 'slopes': 721, 'radio': 722, 'lunch': 723, 'vegetable': 724, 'eggs': 725, 'beans': 726, 'throw': 727, 'residential': 728, 'spots': 729, 'competing': 730, 'multi': 731, 'heavily': 732, 'snowboarder': 733, 'artistic': 734, 'roses': 735, 'displayed': 736, 'blocking': 737, 'tag': 738, 'ear': 739, 'ferris': 740, 'wheel': 741, 'clocks': 742, 'zones': 743, 'driven': 744, 'where': 745, 'rides': 746, 'cone': 747, 'topped': 748, 'forks': 749, 'smelling': 750, 'relish': 751, 'hummingbird': 752, 'flight': 753, 'feeder': 754, 'sleepy': 755, 'slowing': 756, 'passes': 757, 'stabbed': 758, 'washing': 759, 'barbecue': 760, 'lollipops': 761, 'raises': 762, 'students': 763, 'tables': 764, 'books': 765, 'climb': 766, 'mall': 767, 'pants': 768, 'hotel': 769, \"city's\": 770, 'shoreline': 771, 'japan': 772, 'business': 773, 'decorated': 774, 'wallpaper': 775, 'bit': 776, 'skittish': 777, 'dalmatian': 778, 'i': 779, 'am': 780, 'sure': 781, 'mixture': 782, 'workmen': 783, 'concerning': 784, 'sports': 785, 'plentiful': 786, 'work': 787, 'buttermilk': 788, 'faucet': 789, 'toaster': 790, 'neck': 791, 'eat': 792, 'holds': 793, 'tea': 794, 'mates': 795, 'when': 796, '22': 797, 'apartment': 798, 'plant': 799, 'arrange': 800, \"child's\": 801, 'partly': 802, 'ramp': 803, 'laces': 804, 'trail': 805, 'battle': 806, 'trash': 807, 'headset': 808, 'cane': 809, 'grilled': 810, 'spears': 811, 'loaded': 812, 'blows': 813, 'hulk': 814, 'lizard': 815, 'biker': 816, 'bald': 817, 'vest': 818, 'straight': 819, 'ledge': 820, 'sleek': 821, 'ceramic': 822, 'carafe': 823, 'terracotta': 824, 'planter': 825, 'worn': 826, 'drawer': 827, 'spinach': 828, 'stethoscope': 829, 'shoulder': 830, 'wilderness': 831, 'lift': 832, 'operating': 833, 'telephone': 834, 'bath': 835, 'lving': 836, 'here': 837, 'drinks': 838, 'electrical': 839, 'detour': 840, 'traditional': 841, 'atop': 842, 'underwear': 843, 'cooking': 844, 'alarm': 845, 'digital': 846, 'well': 847, 'pressing': 848, 'clydesdale': 849, 'wagon': 850, 'parade': 851, 'states': 852, 'bordering': 853, 'passing': 854, 'cage': 855, 'peaking': 856, 'center': 857, \"batter's\": 858, 'umpire': 859, 'barn': 860, 'these': 861, 'tagged': 862, 'barb': 863, 'wire': 864, 'pc': 865, 'mounted': 866, 'same': 867, \"1800's\": 868, 'fabric': 869, 'cord': 870, 'stained': 871, 'curtains': 872, 'rust': 873, 'kiosk': 874, 'rescue': 875, 'inclosure': 876, 'presented': 877, 'arid': 878, 'plain': 879, 'arena': 880, 'bite': 881, 'icy': 882, 'cave': 883, 'teenager': 884, 'pan': 885, 'seductive': 886, 'cleaner': 887, 'plaid': 888, 'int': 889, 'siting': 890, 'similar': 891, 'hole': 892, 'canoe': 893, 'batting': 894, 'pans': 895, 'asparagus': 896, 'mixed': 897, 'khaki': 898, 'teenagers': 899, 'fellow': 900, 'whipped': 901, 'carpet': 902, 'corridor': 903, 'toyota': 904, 'dealership': 905, 'wading': 906, 'learning': 907, 'how': 908, 'laid': 909, 'snowboarding': 910, 'wooded': 911, 'flip': 912, 'stools': 913, 'backlit': 914, 'aerial': 915, 'stunt': 916, 'flat': 917, 'written': 918, 'israeli': 919, 'depicts': 920, 'knitting': 921, 'onto': 922, 'craft': 923, 'project': 924, 'both': 925, 'afternoon': 926, 'located': 927, 'pulled': 928, 'drives': 929, 'attached': 930, 'hydraulic': 931, 'statue': 932, 'lap': 933, 'aircraft': 934, 'doll': 935, 'laps': 936, 'five': 937, 'containers': 938, \"there's\": 939, 'floral': 940, 'tissues': 941, 'pocket': 942, 'kneels': 943, 'pool': 944, 'model': 945, 'fuzzy': 946, 'slip': 947, 'fashioned': 948, 'prairie': 949, 'contains': 950, 'doors': 951, 'oval': 952, 'racks': 953, 'slop': 954, 'gnome': 955, 'face': 956, 'camo': 957, 'wait': 958, 'almost': 959, 'palm': 960, 'beanie': 961, 'panda': 962, 'placed': 963, 'ripe': 964, 'toward': 965, 'best': 966, 'filled': 967, 'stir': 968, 'fried': 969, 'american': 970, 'flag': 971, 'desert': 972, 'fruit': 973, 'toothbrush': 974, 'makes': 975, 'blt': 976, 'sloppy': 977, 'joe': 978, 'brownie': 979, 'remains': 980, 'shaped': 981, 'opposing': 982, 'moped': 983, 'named': 984, 'minor': 985, 'tossing': 986, 'diamond': 987, 'tabletop': 988, 'thar': 989, 'skining': 990, 'juice': 991, 'whole': 992, 'practice': 993, 'empire': 994, 'spoon': 995, \"gino's\": 996, 'graze': 997, 'sunlit': 998, 'wide': 999, 'texting': 1000, 'prepared': 1001, 'crow': 1002, 'closet': 1003, 'claw': 1004, 'position': 1005, 'hovers': 1006, 'handles': 1007, 'gloves': 1008, 'moored': 1009, 'military': 1010, 'fueling': 1011, 'jets': 1012, 'visible': 1013, 'bundle': 1014, 'piles': 1015, 'bending': 1016, 'shirtless': 1017, 'jumped': 1018, 'friend': 1019, 'advertisements': 1020, 'skating': 1021, 'monopoly': 1022, 'donkey': 1023, 'mat': 1024, 'containing': 1025, 'chest': 1026, 'us': 1027, 'airways': 1028, 'taxi': 1029, 'gate': 1030, 'listening': 1031, 'headphones': 1032, 'photographers': 1033, 'amateur': 1034, 'polar': 1035, 'lolls': 1036, 'beat': 1037, 'heat': 1038, 'activity': 1039, 'bags': 1040, 'service': 1041, 'steep': 1042, 'resting': 1043, 'heard': 1044, 'fours': 1045, 'skies': 1046, 'poles': 1047, 'hello': 1048, 'kitty': 1049, 'chopped': 1050, 'threw': 1051, 'posted': 1052, 'construction': 1053, 'barrels': 1054, 'beverages': 1055, 'rose': 1056, 'bush': 1057, 'mound': 1058, 'lays': 1059, 'go': 1060, 'nearby': 1061, 'tusks': 1062, 'reaching': 1063, 'maker': 1064, 'dispenser': 1065, 'desserts': 1066, 'knife': 1067, 'indoors': 1068, 'coordinating': 1069, 'flown': 1070, 'disrepair': 1071, 'twin': 1072, 'headboard': 1073, '3': 1074, 'regular': 1075, 'bedspread': 1076, 'ridgewood': 1077, 'charles': 1078, 'ave': 1079, 'jerseys': 1080, '\\n': 1081, 'cubs': 1082, 'flags': 1083, 'enclosed': 1084, 'rug': 1085, 'wit': 1086, 'ha': 1087, 'streets': 1088, 'post': 1089, \"pirate's\": 1090, 'alley': 1091, 'put': 1092, 'guzzled': 1093, 'do': 1094, 'sets': 1095, 'camping': 1096, 'equipment': 1097, 'bumper': 1098, 'held': 1099, 'leads': 1100, 'passageway': 1101, 'stable': 1102, 'hugs': 1103, 'chocolate': 1104, 'eyes': 1105, 'motor': 1106, 'cycle': 1107, 'starring': 1108, 'finger': 1109, 'approaching': 1110, 'suits': 1111, 'skateboarders': 1112, 'file': 1113, 'rhino': 1114, \"someone's\": 1115, 'shutters': 1116, 'bars': 1117, 'outfielder': 1118, 'outfield': 1119, 'gooey': 1120, 'smalls': 1121, 'lay': 1122, 'nectarine': 1123, 'stick': 1124, 'ironing': 1125, 'lead': 1126, 'race': 1127, 'uniform': 1128, 'second': 1129, 'scaffolding': 1130, 'ceiling': 1131, 'story': 1132, 'lettuce': 1133, 'square': 1134, 'dinning': 1135, 'buttons': 1136, 'lid': 1137, 'plymstock': 1138, 'daily': 1139, 'schedule': 1140, 'army': 1141, 'thin': 1142, 'restaurant': 1143, 'hangar': 1144, 'runs': 1145, 'gets': 1146, 'dinner': 1147, 'antique': 1148, 'telling': 1149, 'photos': 1150, 'gathering': 1151, 'reads': 1152, '12': 1153, '08pm': 1154, 'loading': 1155, 'cabinet': 1156, 'typical': 1157, 'pantry': 1158, 'movies': 1159, 'seats': 1160, 'vases': 1161, 'plaza': 1162, 'nutritious': 1163, 'hearty': 1164, 'breakfast': 1165, 'homes': 1166, 'facing': 1167, 'canada': 1168, 'ride': 1169, \"somebody's\": 1170, 'cookware': 1171, 'airplanes': 1172, 'dress': 1173, 'cap': 1174, 'grabbing': 1175, 'jump': 1176, 'derby': 1177, 'waveland': 1178, 'avenue': 1179, 'word': 1180, 'suffolk': 1181, 'jar': 1182, 'grocery': 1183, 'plowing': 1184, 'oxen': 1185, 'sneakers': 1186, 'match': 1187, 'monument': 1188, 'sides': 1189, 'steam': 1190, 'flies': 1191, 'urinals': 1192, 'island': 1193, 'featured': 1194, 'lettering': 1195, 'wears': 1196, 'slicing': 1197, 'players': 1198, 'net': 1199, 'scenic': 1200, 'lots': 1201, 'jewelry': 1202, 'peeling': 1203, 'tones': 1204, 'passengers': 1205, 'tablecloth': 1206, 'order': 1207, 'surprise': 1208, 'party': 1209, 'tickets': 1210, 'motorcycles': 1211, 'should': 1212, 'cleaned': 1213, 'prior': 1214, 'any': 1215, 'backyard': 1216, 'taller': 1217, 'than': 1218, 'surround': 1219, 'unopened': 1220, 'pastry': 1221, 'reflection': 1222, 'mushroom': 1223, 'if': 1224, 'swept': 1225, 'broom': 1226, 'stuff': 1227, 'pandas': 1228, 'prizes': 1229, 'carnival': 1230, 'kitten': 1231, 'peopler': 1232, 'catching': 1233, 'sat': 1234, 'used': 1235, 'variety': 1236, 'dead': 1237, 'sparkling': 1238, 'ergonomic': 1239, 'rests': 1240, 'sailboats': 1241, 'marina': 1242, 'railroad': 1243, 'houses': 1244, 'naked': 1245, 'stretches': 1246, 'culminating': 1247, 'stunning': 1248, 'backdrop': 1249, 'evergreen': 1250, 'forest': 1251, 'jockey': 1252, 'marquee': 1253, 'still': 1254, 'process': 1255, 'making': 1256, 'strip': 1257, 'monitors': 1258, 'mid': 1259, 'chefs': 1260, 'bookstore': 1261, 'painted': 1262, 'beautiful': 1263, 'graffiti': 1264, 'among': 1265, 'stall': 1266, 'displays': 1267, 'noodles': 1268, 'habitat': 1269, 'waits': 1270, 'scissorss': 1271, 'sport': 1272, 'public': 1273, 'transportation': 1274, 'checks': 1275, 'phones': 1276, 'ridge': 1277, 'sandy': 1278, 'painting': 1279, 'you': 1280, 'breed': 1281, 'desktop': 1282, 'thought': 1283, 'been': 1284, 'yet': 1285, 'mans': 1286, 'font': 1287, 'traffic': 1288, 'knit': 1289, 'apartments': 1290, 'such': 1291, 'pot': 1292, 'china': 1293, 'airlines': 1294, 'marked': 1295, 'strawberrys': 1296, 'institutional': 1297, 'retail': 1298, 'sauces': 1299, 'container': 1300, 'curry': 1301, 'ketchup': 1302, 'smothered': 1303, 'bricks': 1304, 'gold': 1305, 't': 1306, 'v': 1307, 'projection': 1308, 'colors': 1309, 'make': 1310, 'southwest': 1311, 'easily': 1312, 'discernable': 1313, 'cracked': 1314, 'also': 1315, 'rotten': 1316, 'pile': 1317, 'childs': 1318, 'doo': 1319, 'rags': 1320, 'adults': 1321, 'buoy': 1322, 'ships': 1323, 'bathrobe': 1324, 'cruise': 1325, 'kingston': 1326, 'philippines': 1327, 'architectural': 1328, 'bathing': 1329, 'grassland': 1330, 'thick': 1331, 'tomatoes': 1332, 'melon': 1333, 'sack': 1334, 'grapes': 1335, 'cucumbers': 1336, 'youngster': 1337, 'fancy': 1338, 'reflects': 1339, 'cupboards': 1340, 'objects': 1341, 'crafts': 1342, 'says': 1343, 'ahead': 1344, 'were': 1345, 'body': 1346, 'curly': 1347, 'haired': 1348, 'walls': 1349, 'whose': 1350, 'blood': 1351, 'seem': 1352, 'examining': 1353, 'trunk': 1354, 'consumption': 1355, 'towel': 1356, 'cactus': 1357, 'kneeling': 1358, 'feeding': 1359, 'trough': 1360, 'peeking': 1361, 'thru': 1362, 'remote': 1363, 'mug': 1364, 'shiny': 1365, 'sparrow': 1366, 'rider': 1367, 'scarf': 1368, 'engines': 1369, 'forward': 1370, 'stating': 1371, 'pond': 1372, 'bow': 1373, 'ultimate': 1374, 'uniformed': 1375, 'followed': 1376, 'fall': 1377, 'grand': 1378, 'canyon': 1379, 'fell': 1380, 'shaved': 1381, 'engine': 1382, 'perches': 1383, \"man's\": 1384, 'tan': 1385, 'motorcyclists': 1386, 'highway': 1387, 'chihuahua': 1388, 'colorful': 1389, 'hseet': 1390, 'departure': 1391, 'highchair': 1392, 'healthy': 1393, 'dairy': 1394, 'pancakes': 1395, 'jeep': 1396, 'blurred': 1397, 'posture': 1398, 'boards': 1399, 'money': 1400, 'pug': 1401, 'thermos': 1402, 'feather': 1403, 'levels': 1404, 'mac': 1405, 'devices': 1406, 'farmers': 1407, 'watermelon': 1408, 'bell': 1409, 'squash': 1410, \"nathan's\": 1411, 'get': 1412, 'numerous': 1413, 'indicate': 1414, 'pedestrians': 1415, \"ocean's\": 1416, 'lamp': 1417, 'spotlight': 1418, 'creates': 1419, 'oil': 1420, 'utensils': 1421, 'goblet': 1422, 'fruits': 1423, 'blender': 1424, 'rancher': 1425, 'horseback': 1426, 'leading': 1427, 'path': 1428, 'carts': 1429, 'wrench': 1430, 'adjusts': 1431, 'rocky': 1432, 'cliff': 1433, 'stool': 1434, 'dresser': 1435, 'selling': 1436, 'wares': 1437, 'scale': 1438, 'bar': 1439, 'notes': 1440, 'glaze': 1441, 'sprinkles': 1442, 'rainbow': 1443, 'drying': 1444, 'sailors': 1445, 'device': 1446, 'bends': 1447, 'stares': 1448, 'skatepark': 1449, 'meters': 1450, 'german': 1451, 'shepherd': 1452, 'criss': 1453, 'system': 1454, 'rushing': 1455, 'infant': 1456, 'barrier': 1457, 'layers': 1458, 'bride': 1459, 'groom': 1460, 'magnets': 1461, 'others': 1462, 'dashboard': 1463, 'someones': 1464, 'vehicle': 1465, 'motel': 1466, 'low': 1467, 'kissing': 1468, 'place': 1469, 'greek': 1470, 'wheat': 1471, 'buggy': 1472, 'fighting': 1473, 'vanity': 1474, 'folded': 1475, 'ovens': 1476, 'stair': 1477, 'case': 1478, 'away': 1479, 'pavement': 1480, 'speed': 1481, '<pad>': 0}\n",
            "41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_654_vjoOkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "# Purpose: Split the data into validation and training with split percentage   #\n",
        "# of 10%                                                                       #\n",
        "# Author: Akanksha Malhotra                                                    #\n",
        "################################################################################\n",
        "img_name_train, img_name_val, question_train, question_val,answer_train, answer_val, explaination_train, explaination_val, all_comp, all_comp_val  = train_test_split(img_name_vector,\n",
        "                                                                    question_vector,\n",
        "                                                                    answer_vector,explaination_vector, all_complementary_pairs,\n",
        "                                                                    test_size=0.1,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTf5r4PkoOmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_func(img_name, ques,ans,exp):\n",
        "  \"\"\"\n",
        "    Uses cv2 to read the image and resize it\n",
        "    Creates one hot encoder for answers and explanation vector\n",
        "    Author: Akanksha Malhotra\n",
        "    :param:  image_name: path to the image, ques: ques_vector, ans: answer, \n",
        "             exp: explanation.\n",
        "    :return: image_tensor, question, answer and explanation\n",
        "    \"\"\"\n",
        "  img_tensor = np.array(cv2.resize(cv2.imread(img_name, cv2.IMREAD_UNCHANGED),(224,224)), dtype=np.float32)\n",
        "  ans =  tf.one_hot(ans,depth=len(all_answers))\n",
        "  exp =  [tf.one_hot(j,depth=explaination_vocab) for j in exp]\n",
        "  return(img_tensor,ques, ans, exp)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c1PbEYnCoyI",
        "colab_type": "code",
        "outputId": "a1bd994d-25fd-4a5c-8e38-e0233ff0c0df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def pretrained_embeddings(file_path, embedding_dim, vocab_Size, word2idx):\n",
        "  '''\n",
        "   1. load in pre-trained word vectors\n",
        "   2. Creates embedding matrix for the given vocab using word2idx\n",
        "   Author: Akanksha Malhotra\n",
        "    :param:  file_path, embedding_dim, vocab_Size, word2idx\n",
        "    :return: embedding matrix\n",
        "  '''     \n",
        "  glove_vectors = load_glove_model(file_path)\n",
        "  print('Found %s word vectors.' % len(glove_vectors))\n",
        "  print('Filling pre-trained embeddings...')\n",
        "  num_words = vocab_Size\n",
        "  # initialization by zeros\n",
        "  embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "  for word, i in word2idx.items():\n",
        "    if i < vocab_Size:\n",
        "        embedding_vector = glove_vectors.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n",
        "      \n",
        "ques_embedding_matrix = pretrained_embeddings(\"/content/drive/My Drive/glove.6B.300d\", 300, ques_vocab, tokenizer.word_index)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400001 word vectors.\n",
            "Filling pre-trained embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EhQ1wf7233X",
        "colab_type": "code",
        "outputId": "e38af676-076b-4c7f-ef38-eb87724fe4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "########################################################\n",
        "# 1. Read Image                                        #\n",
        "# 2. Create One-hot vector for answer and explanation. #\n",
        "#                                                      #\n",
        "# Author: Akanksha Malhotra                            # \n",
        "########################################################\n",
        "with strategy.scope():\n",
        "    ans_val = []\n",
        "    wierd_img = []\n",
        "    image_tensor_val = []\n",
        "    ques_val = []\n",
        "    exp_val = []\n",
        "    for i in tqdm(range(len(img_name_val))):\n",
        "      l = map_func(img_name_val[i], question_val[i], answer_val[i],explaination_val[i])\n",
        "      if len(l[0].shape) ==  3:\n",
        "        ans_val.append(l[2])\n",
        "        image_tensor_val.append(l[0])\n",
        "        ques_val.append(l[1]) \n",
        "        exp_val.append(l[3])\n",
        "      else:\n",
        "        wierd_img.append(i)\n",
        "    # wierd_img = []\n",
        "    ans = []\n",
        "    image_tensor = []\n",
        "    ques = []\n",
        "    exp = []\n",
        "\n",
        "    for i in tqdm(range(len(img_name_train))):\n",
        "      l = map_func(img_name_train[i], question_train[i], answer_train[i],explaination_train[i])\n",
        "      if len(l[0].shape) ==  3:\n",
        "        ans.append(l[2])\n",
        "        image_tensor.append(l[0])\n",
        "        ques.append(l[1]) \n",
        "        exp.append(l[3])\n",
        "      else:\n",
        "        wierd_img.append(i)\n",
        "\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 49.92it/s]\n",
            "100%|██████████| 900/900 [00:18<00:00, 48.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0kxe_ePmc2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################################\n",
        "# 1. The dataset for training is created.              #\n",
        "# 2. The dataset for validation  is created.           #\n",
        "#                                                      #\n",
        "# Author: Akanksha Malhotra                            # \n",
        "########################################################\n",
        "# dataset = tf.data.Dataset.from_tensor_slices(({\"ques\": ques, \"img\":  image_tensor}, {\"ans\":ans,\"exp\":exp}))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(({\"ques\": ques, \"img\":  image_tensor}, {\"ans\":ans,\"exp\":exp}))\n",
        "BATCH_SIZE = 1 #2 #64\n",
        "BUFFER_SIZE = 300 #1000\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices(({\"ques\": ques_val, \"img\":  image_tensor_val}, {\"ans\":ans_val,\"exp\":exp_val}))\n",
        "# val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF6reebViryr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6cae88cd-ec19-4266-a09d-7f9aa93a802c"
      },
      "source": [
        "################################################################################\n",
        "#  1. As the model has two inputs, two seperate input channels are created for #\n",
        "#     tensorflow functional model.                                             # \n",
        "#     a. input1: Denotes the question tensor vector                            #\n",
        "#     b. input2: Denotes the image tensor vector                               #\n",
        "#  2. The pretrained model for images is loaded, we use ResNet152              #\n",
        "#  3. Layers are created for the model                                         #\n",
        "#        Question Layer: Embedding Layer, GRU and a Dense Layer                #\n",
        "#        Image Layer: ResNet152 base model and a Dense Layer                   #\n",
        "#        Attention: Attention Layer, Dense Layer, Hadamard Product             #\n",
        "#        Answer generation: Attention , softmax                                #\n",
        "#        Explanation generation: attention, dense, embedding layer, gru, dense #\n",
        "#        with units equal to gru units and another dense layer with units equal#\n",
        "#        to explanation vocab                                                  #\n",
        "#  4. Compiled Model, with two losses                                          #\n",
        "#  5. Model Fit\n",
        "#  Author: Akanksha Malhotra                                                   #    \n",
        "################################################################################ \n",
        "\n",
        "input1 =  keras.Input(shape=(max_q), name=\"ques\", batch_size = BATCH_SIZE)\n",
        "input2 =  keras.Input(shape=(224, 224, 3), name=\"img\", batch_size = BATCH_SIZE)\n",
        "\n",
        "print(\"read inputs\")\n",
        "\n",
        "IMG_SHAPE = (224, 224, 3)\n",
        "base_model = keras.applications.ResNet152(input_shape=IMG_SHAPE, include_top=False, weights='imagenet', pooling='avg')\n",
        "base_model.trainable = False\n",
        "\n",
        "print(\"Basel Model of ResNet152 loaded\")\n",
        "\n",
        "embedding_dim = 300\n",
        "enc_units = 1024\n",
        "ques_model = layers.Embedding(ques_vocab, embedding_dim, weights=[ques_embedding_matrix],  trainable=False, input_length=max_q)(input1)\n",
        "ques_model = layers.GRU(enc_units,recurrent_initializer='glorot_uniform')(ques_model)\n",
        "ques_model = layers.Dense(1024, activation='relu')(ques_model)\n",
        "img_model = base_model(input2)\n",
        "img_model = layers.Dense(units=1024, activation=\"relu\")(img_model)\n",
        "attention = layers.Attention()([img_model, img_model, ques_model])\n",
        "attention = layers.Dense(1024, activation='relu')(attention)\n",
        "attention_output = layers.Multiply()([ques_model,attention])\n",
        "at = layers.Dense(max_length1, activation='relu')(attention_output)\n",
        "predicted_output = layers.Dense(1024,\"relu\")(attention_output)\n",
        "predicted_output = layers.Dense(len(all_answers),\"softmax\",name=\"ans\")(predicted_output)\n",
        "emb = tf.keras.layers.Embedding(max_length1, embedding_dim)(at)\n",
        "predicted_explaination = layers.GRU(enc_units,recurrent_initializer='glorot_uniform', return_sequences=True)(emb)\n",
        "predicted_explaination = layers.Dense(enc_units)(predicted_explaination)\n",
        "predicted_explaination1 = layers.Dense(explaination_vocab, name=\"exp\")(predicted_explaination)\n",
        "\n",
        "print(\"Model Architecture Created\")\n",
        "\n",
        "model = tf.keras.Model(inputs=[input1,input2], outputs=[predicted_output,predicted_explaination1])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "losses = {'ans':tf.losses.categorical_crossentropy, 'exp': tf.losses.categorical_crossentropy}\n",
        "model.compile(loss=losses,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Model Compiled\")\n",
        "\n",
        "model.fit(dataset, epochs=50, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read inputs\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234700800/234698864 [==============================] - 1s 0us/step\n",
            "Basel Model of ResNet152 loaded\n",
            "Model Architecture Created\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "ques (InputLayer)               [(1, 13)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (1, 13, 300)         1500300     ques[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "img (InputLayer)                [(1, 224, 224, 3)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       (1, 1024)            4073472     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "resnet152 (Model)               multiple             58370944    img[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (1, 1024)            1049600     gru[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (1, 1024)            2098176     resnet152[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "attention (Attention)           (1, 1024)            0           dense_1[0][0]                    \n",
            "                                                                 dense_1[0][0]                    \n",
            "                                                                 dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (1, 1024)            1049600     attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (1, 1024)            0           dense[0][0]                      \n",
            "                                                                 dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (1, 41)              42025       multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (1, 41, 300)         12300       dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (1, 41, 1024)        4073472     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (1, 1024)            1049600     multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (1, 41, 1024)        1049600     gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "ans (Dense)                     (1, 3180)            3259500     dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "exp (Dense)                     (1, 41, 5001)        5126025     dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 82,754,614\n",
            "Trainable params: 22,883,370\n",
            "Non-trainable params: 59,871,244\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model Compiled\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "898/898 [==============================] - 229s 254ms/step - loss: 18.6856 - ans_loss: 4.8611 - exp_loss: 13.8245 - ans_accuracy: 0.2572 - exp_accuracy: 0.1817\n",
            "Epoch 2/50\n",
            "608/898 [===================>..........] - ETA: 1:12 - loss: 16.1035 - ans_loss: 3.6512 - exp_loss: 12.4524 - ans_accuracy: 0.2467 - exp_accuracy: 0.1126"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkBJch0wC0zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/VQA - Project/data/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/VQA - Project/data/model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC5Gx7qNN1K1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}